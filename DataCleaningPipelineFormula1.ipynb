{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ab1759-b085-4b3a-80b4-e802d1ee6aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/21 14:58:54 WARN Utils: Your hostname, Divyanshs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.33.74.20 instead (on interface en0)\n",
      "25/11/21 14:58:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/21 14:58:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, trim, lower, when, coalesce, first, last,\n",
    "    concat_ws, concat, udf, collect_list, array_join, explode,\n",
    "    min as spark_min, max as spark_max, expr\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# ========== 1. Start spark ==========\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F1 Pitstop Pipeline 2018-2024\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb439eb3-ac44-45f6-b726-75dcda84c22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "doneecting package metadata (repodata.json): - \n",
      "doneing environment: / \n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.9.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge openjdk=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add63462-187e-4ddb-ac20-65b60651c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. Paths (update to your repo / S3 / HDFS) ==========\n",
    "RAW_DIR = \"./data/raw\"           # relative to current directory\n",
    "OUT_DIR = \"./data/processed\"     # final outputs here\n",
    "\n",
    "# example files:\n",
    "PIT1_PATH = f\"{RAW_DIR}/pitstop_1st.csv\"\n",
    "PIT2_PATH = f\"{RAW_DIR}/pitstop_2nd.csv\"\n",
    "SC_PATH   = f\"{RAW_DIR}/safety_cars.csv\"\n",
    "RF_PATH   = f\"{RAW_DIR}/red_flags.csv\"\n",
    "\n",
    "# optional lookups (you should download from Ergast/Kaggle or supply them)\n",
    "RACES_PATH   = f\"{RAW_DIR}/races.csv\"    # maps raceId -> Season, Round, RaceName, Date\n",
    "DRIVERS_PATH = f\"{RAW_DIR}/drivers.csv\"  # maps driverId -> driverName, abbreviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc51d47-013b-4aef-8137-6cbc5a6aa888",
   "metadata": {},
   "source": [
    "Now Checking if the files are loaded correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7376f639-5e45-4bf5-ba05-be65182b5a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/divyanshdoshi/Documents/GitHub/Cloud_Formula1_Data_Cleaning_Pipeline\n",
      "\n",
      "Checking file existence:\n",
      "yes Pitstops 1st: ./data/raw/pitstop_1st.csv\n",
      "yes Pitstops 2nd: ./data/raw/pitstop_2nd.csv\n",
      "yes Safety Car: ./data/raw/safety_cars.csv\n",
      "yes Red Flag: ./data/raw/red_flags.csv\n",
      "yes Races Lookup: ./data/raw/races.csv\n",
      "yes Drivers Lookup: ./data/raw/drivers.csv\n",
      "\n",
      "Contents of ./data/raw:\n",
      "  - pitstop_1st.csv\n",
      "  - safety_cars.csv\n",
      "  - drivers.csv\n",
      "  - red_flags.csv\n",
      "  - races.csv\n",
      "  - pitstop_2nd.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nChecking file existence:\")\n",
    "files_to_check = {\n",
    "    \"Pitstops 1st\": PIT1_PATH,\n",
    "    \"Pitstops 2nd\": PIT2_PATH,\n",
    "    \"Safety Car\": SC_PATH,\n",
    "    \"Red Flag\": RF_PATH,\n",
    "    \"Races Lookup\": RACES_PATH,\n",
    "    \"Drivers Lookup\": DRIVERS_PATH\n",
    "}\n",
    "\n",
    "for name, path in files_to_check.items():\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"{'yes' if exists else 'no'} {name}: {path}\")\n",
    "    \n",
    "# Also check the raw directory contents\n",
    "print(f\"\\nContents of {RAW_DIR}:\")\n",
    "try:\n",
    "    if os.path.exists(RAW_DIR):\n",
    "        for item in os.listdir(RAW_DIR):\n",
    "            print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(f\"Directory {RAW_DIR} does not exist!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fa805-d70b-4796-b0f9-cfdd0975979b",
   "metadata": {},
   "source": [
    "Now We can see that the dataset are correctly loaded now we can start cleaning and joining the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc448c-2759-41c1-a172-486249d020a9",
   "metadata": {},
   "source": [
    "Our main aim of this Pipeline is to get out clean pitstop there are many problems with the dataset like name issues and no data in someplace so we will tackle this using spark \n",
    "And for our final aim to use this data for predicting better pitstop strategy we all need to know when the pitstop was done under safety car which can change whole race dynamic and we have to change the whole race strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a1dca43-343d-4b5f-9cfd-9304cdb75d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use inferSchema=False and define schema or let Spark infer; define options to handle encoding.\n",
    "pit2 = spark.read.options(header=True, multiLine=True, escape='\"').csv(PIT2_PATH)\n",
    "pit1 = spark.read.options(header=True, multiLine=True, escape='\"').csv(PIT1_PATH)\n",
    "safety = spark.read.options(header=True).csv(SC_PATH)\n",
    "redflag = spark.read.options(header=True).csv(RF_PATH)\n",
    "\n",
    "# Optional lookups (if present)\n",
    "races_df = None\n",
    "drivers_df = None\n",
    "try:\n",
    "    races_df = spark.read.options(header=True).csv(RACES_PATH)\n",
    "    drivers_df = spark.read.options(header=True).csv(DRIVERS_PATH)\n",
    "except Exception:\n",
    "    print(\"Lookups not provided yet; pipeline will attempt fuzzy joins or require you to add lookups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a777692-b69e-4088-a13b-55761e200a5c",
   "metadata": {},
   "source": [
    "This code loads your raw data files (pit stops, safety car, red flag data, races, drivers) into Spark DataFrames so we can process them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4578c1-b8f0-4a6a-9199-fa7e5f2b9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(df, col_names):\n",
    "    # Trim whitespace and convert empty strings to null\n",
    "    for c in col_names:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "            df = df.withColumn(c, when(col(c) == \"\", None).otherwise(col(c)))\n",
    "    return df\n",
    "\n",
    "pit2 = basic_clean(pit2, pit2.columns)\n",
    "pit1 = basic_clean(pit1, pit1.columns)\n",
    "safety = basic_clean(safety, safety.columns)\n",
    "redflag = basic_clean(redflag, redflag.columns)\n",
    "\n",
    "encoding_fixes = {\n",
    "    \"Kimi R√É∆í√Ç¬§ikk√É∆í√Ç¬∂nen\": \"Kimi Räikkönen\",\n",
    "    \"RÃ¤ikkÃ¶nen\": \"Kimi Räikkönen\"\n",
    "}\n",
    "fix_udf = F.udf(lambda s: encoding_fixes.get(s, s) if s is not None else None, StringType())\n",
    "if \"Driver\" in pit2.columns:\n",
    "    pit2 = pit2.withColumn(\"Driver\", fix_udf(col(\"Driver\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1879ba7-8229-4350-87b4-1c05666fa5de",
   "metadata": {},
   "source": [
    "this code help us remove text gaps and messy names of the drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30aaebfd-29bd-4237-896b-da4201c6f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_casts = {\n",
    "    \"Season\": IntegerType(),\n",
    "    \"Round\": IntegerType(),\n",
    "    \"Laps\": IntegerType(),\n",
    "    \"Position\": IntegerType(),\n",
    "    \"TotalPitStops\": IntegerType(),\n",
    "    \"Stint\": IntegerType(),\n",
    "    \"Stint Length\": IntegerType(),  # if present; watch spaces\n",
    "    \"Stint Length\": IntegerType()\n",
    "}\n",
    "\n",
    "# cast safe function\n",
    "def safe_cast(df, colname, dtype):\n",
    "    if colname in df.columns:\n",
    "        try:\n",
    "            return df.withColumn(colname, col(colname).cast(dtype))\n",
    "        except Exception:\n",
    "            return df\n",
    "    return df\n",
    "\n",
    "# cast common numeric columns in pit2\n",
    "for c in [\"Season\", \"Round\", \"Laps\", \"Position\", \"TotalPitStops\", \"Stint\", \"Stint Length\", \"StintLength\", \"Pit_Lap\", \"Stint Length\"]:\n",
    "    if c in pit2.columns:\n",
    "        pit2 = pit2.withColumn(c, col(c).cast(IntegerType()))\n",
    "\n",
    "# cast pit1 numeric columns\n",
    "for c in [\"raceId\", \"driverId\", \"stop\", \"lap\", \"milliseconds\"]:\n",
    "    if c in pit1.columns:\n",
    "        pit1 = pit1.withColumn(c, col(c).cast(IntegerType()))\n",
    "for c in [\"duration\"]:\n",
    "    if c in pit1.columns:\n",
    "        # duration may be text like '26.898'\n",
    "        pit1 = pit1.withColumn(c, col(c).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb312033-f023-41b6-9ffc-bb42024806f8",
   "metadata": {},
   "source": [
    "this code helps us cover text into numeric form so that it can be used for proper analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b53c4c-2817-43f1-8331-ab3e4b85249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_non_null(arr):\n",
    "    if arr is None:\n",
    "        return None\n",
    "    for x in arr:\n",
    "        if x is not None and str(x).strip() != \"\":\n",
    "            return x\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df4f278b-7fa0-4d33-830a-f6c019f8efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_non_null_udf = F.udf(first_non_null, StringType())\n",
    "\n",
    "group_cols = []\n",
    "for c in [\"Season\",\"Round\",\"Circuit\",\"Driver\",\"Constructor\",\"Race Name\",\"Date\",\"Location\",\"Country\",\"Abbreviation\"]:\n",
    "    if c in pit2.columns:\n",
    "        group_cols.append(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78bdc55-11c7-4b57-b940-e67cc7cf9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_exprs = []\n",
    "# gather lists for candidate columns\n",
    "candidates = [\"Pit_Lap\", \"Pit_Time\", \"Tire Compound\", \"Stint Length\", \"Pit_Time\", \"Pit_Lap\", \"Pit_Time\"]\n",
    "for cand in set([c for c in candidates if c in pit2.columns]):\n",
    "    agg_exprs.append(F.collect_list(cand).alias(f\"{cand}_list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b49d00-e12a-46f9-a4ba-3d420b580413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also keep numeric environmental aggregates (Air_Temp_C etc) -> take first\n",
    "for env in [\"Air_Temp_C\",\"Track_Temp_C\",\"Humidity_%\",\"Wind_Speed_KMH\",\"AvgPitStopTime\"]:\n",
    "    if env in pit2.columns:\n",
    "        agg_exprs.append(first(env).alias(env))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa0b8a5-e78f-4bd9-82ab-e65cead958a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build grouping\n",
    "if len(group_cols) == 0:\n",
    "    raise Exception(\"Expected at least one grouping column in pit2 (Season/Round/Driver). Check file headers.\")\n",
    "\n",
    "agg_df = pit2.groupBy(*group_cols).agg(*agg_exprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00306100-81e6-4cde-81b1-f344fc8f9e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in agg_df:\n",
      "  - Season\n",
      "  - Round\n",
      "  - Circuit\n",
      "  - Driver\n",
      "  - Constructor\n",
      "  - Race Name\n",
      "  - Date\n",
      "  - Location\n",
      "  - Country\n",
      "  - Abbreviation\n",
      "  - Stint Length_list\n",
      "  - Pit_Lap_list\n",
      "  - Pit_Time_list\n",
      "  - Tire Compound_list\n",
      "  - Air_Temp_C\n",
      "  - Track_Temp_C\n",
      "  - Humidity_%\n",
      "  - Wind_Speed_KMH\n",
      "  - AvgPitStopTime\n",
      "\n",
      "Found list columns: ['Stint Length_list', 'Pit_Lap_list', 'Pit_Time_list', 'Tire Compound_list']\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what columns actually exist\n",
    "print(\"Available columns in agg_df:\")\n",
    "for col_name in agg_df.columns:\n",
    "    print(f\"  - {col_name}\")\n",
    "\n",
    "# Only process columns that actually have the \"_list\" suffix\n",
    "existing_list_columns = [col_name for col_name in agg_df.columns if col_name.endswith('_list')]\n",
    "print(f\"\\nFound list columns: {existing_list_columns}\")\n",
    "\n",
    "# Process only the columns that actually exist\n",
    "for list_col in existing_list_columns:\n",
    "    base_col = list_col.replace('_list', '')  # Remove the _list suffix\n",
    "    agg_df = agg_df.withColumn(base_col, first_non_null_udf(col(list_col)))\n",
    "    agg_df = agg_df.drop(list_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da4d271-a953-4c88-bef5-c7fbab7ac5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 15:13:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 6:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+------------+--------------------+----------+---------+---------+------------+-----------------+-----------------+----------+--------------+--------------+-------+--------+-------------+------------+\n",
      "|Season|Round|             Circuit|              Driver| Constructor|           Race Name|      Date| Location|  Country|Abbreviation|       Air_Temp_C|     Track_Temp_C|Humidity_%|Wind_Speed_KMH|AvgPitStopTime|Pit_Lap|Pit_Time|Tire Compound|Stint Length|\n",
      "+------+-----+--------------------+--------------------+------------+--------------------+----------+---------+---------+------------+-----------------+-----------------+----------+--------------+--------------+-------+--------+-------------+------------+\n",
      "|  2018|    1|Albert Park Grand...|     Brendon Hartley|  Toro Rosso|Australian Grand ...|25-03-2018|Melbourne|Australia|         HAR|15.78333333333333|22.28333333333333|        57|          23.8|       22.2545|     22|  22.296|         SOFT|          23|\n",
      "|  2018|    1|Albert Park Grand...|        Carlos Sainz|     Renault|Australian Grand ...|25-03-2018|Melbourne|Australia|         SAI|15.78333333333333|22.28333333333333|        57|          23.8|        23.868|     22|  23.868|    ULTRASOFT|          21|\n",
      "|  2018|    1|Albert Park Grand...|     Charles Leclerc|      Sauber|Australian Grand ...|25-03-2018|Melbourne|Australia|         LEC|15.78333333333333|22.28333333333333|        57|          23.8|        22.539|     20|  22.242|    SUPERSOFT|          19|\n",
      "|  2018|    1|Albert Park Grand...|    Daniel Ricciardo|    Red Bull|Australian Grand ...|25-03-2018|Melbourne|Australia|         RIC|15.78333333333333|22.28333333333333|        57|          23.8|         21.44|     26|   21.44|    SUPERSOFT|          25|\n",
      "|  2018|    1|Albert Park Grand...|        Esteban Ocon| Force India|Australian Grand ...|25-03-2018|Melbourne|Australia|         OCO|15.78333333333333|22.28333333333333|        57|          23.8|        21.854|     23|  21.854|    ULTRASOFT|          22|\n",
      "|  2018|    1|Albert Park Grand...|     Fernando Alonso|     McLaren|Australian Grand ...|25-03-2018|Melbourne|Australia|         ALO|15.78333333333333|22.28333333333333|        57|          23.8|        22.573|     26|  22.573|    ULTRASOFT|          25|\n",
      "|  2018|    1|Albert Park Grand...|     Kevin Magnussen|Haas F1 Team|Australian Grand ...|25-03-2018|Melbourne|Australia|         MAG|15.78333333333333|22.28333333333333|        57|          23.8|        21.983|     22|  21.983|    ULTRASOFT|          21|\n",
      "|  2018|    1|Albert Park Grand...|      Kimi Räikkönen|     Ferrari|Australian Grand ...|25-03-2018|Melbourne|Australia|         RAI|15.78333333333333|22.28333333333333|        57|          23.8|        21.421|     18|  21.421|    ULTRASOFT|          17|\n",
      "|  2018|    1|Albert Park Grand...|        Lance Stroll|    Williams|Australian Grand ...|25-03-2018|Melbourne|Australia|         STR|15.78333333333333|22.28333333333333|        57|          23.8|       23.4505|     25|  25.504|    SUPERSOFT|          24|\n",
      "|  2018|    1|Albert Park Grand...|      Lewis Hamilton|    Mercedes|Australian Grand ...|25-03-2018|Melbourne|Australia|         HAM|15.78333333333333|22.28333333333333|        57|          23.8|        21.821|     19|  21.821|    ULTRASOFT|          17|\n",
      "|  2018|    1|Albert Park Grand...|     Marcus Ericsson|      Sauber|Australian Grand ...|25-03-2018|Melbourne|Australia|         ERI|15.78333333333333|22.28333333333333|        57|          23.8|          NULL|   NULL|    NULL|    SUPERSOFT|           5|\n",
      "|  2018|    1|Albert Park Grand...|      Max Verstappen|    Red Bull|Australian Grand ...|25-03-2018|Melbourne|Australia|         VER|15.78333333333333|22.28333333333333|        57|          23.8|        20.953|     21|  20.953|    SUPERSOFT|          20|\n",
      "|  2018|    1|Albert Park Grand...|Nico H√É∆í√Ç¬ºlke...|     Renault|Australian Grand ...|25-03-2018|Melbourne|Australia|         HUL|15.78333333333333|22.28333333333333|        57|          23.8|        22.628|     24|  22.628|    ULTRASOFT|          23|\n",
      "|  2018|    1|Albert Park Grand...|        Pierre Gasly|  Toro Rosso|Australian Grand ...|25-03-2018|Melbourne|Australia|         GAS|15.78333333333333|22.28333333333333|        57|          23.8|          NULL|   NULL|    NULL|    ULTRASOFT|          13|\n",
      "|  2018|    1|Albert Park Grand...|     Romain Grosjean|Haas F1 Team|Australian Grand ...|25-03-2018|Melbourne|Australia|         GRO|15.78333333333333|22.28333333333333|        57|          23.8|        23.054|     24|  23.054|    ULTRASOFT|          23|\n",
      "|  2018|    1|Albert Park Grand...|    Sebastian Vettel|     Ferrari|Australian Grand ...|25-03-2018|Melbourne|Australia|         VET|15.78333333333333|22.28333333333333|        57|          23.8|        21.787|     26|  21.787|    ULTRASOFT|          25|\n",
      "|  2018|    1|Albert Park Grand...|     Sergey Sirotkin|    Williams|Australian Grand ...|25-03-2018|Melbourne|Australia|         RUS|15.78333333333333|22.28333333333333|        57|          23.8|          NULL|   NULL|    NULL|         NULL|        NULL|\n",
      "|  2018|    1|Albert Park Grand...| Sergio P√É∆í√Ç¬©rez| Force India|Australian Grand ...|25-03-2018|Melbourne|Australia|         PER|15.78333333333333|22.28333333333333|        57|          23.8|        22.327|     24|  22.327|    ULTRASOFT|          23|\n",
      "|  2018|    1|Albert Park Grand...|   Stoffel Vandoorne|     McLaren|Australian Grand ...|25-03-2018|Melbourne|Australia|         BEL|15.78333333333333|22.28333333333333|        57|          23.8|        22.474|   NULL|    NULL|         NULL|        NULL|\n",
      "|  2018|    1|Albert Park Grand...|     Valtteri Bottas|    Mercedes|Australian Grand ...|25-03-2018|Melbourne|Australia|         BOT|15.78333333333333|22.28333333333333|        57|          23.8|        21.664|     25|  21.664|    ULTRASOFT|          24|\n",
      "+------+-----+--------------------+--------------------+------------+--------------------+----------+---------+---------+------------+-----------------+-----------------+----------+--------------+--------------+-------+--------+-------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, first\n",
    "\n",
    "# Group by and aggregate without creating list columns\n",
    "agg_df = pit2.groupBy(\"Season\", \"Round\", \"Circuit\", \"Driver\", \"Constructor\").agg(\n",
    "    first(\"Race Name\").alias(\"Race Name\"),\n",
    "    first(\"Date\").alias(\"Date\"),\n",
    "    first(\"Location\").alias(\"Location\"),\n",
    "    first(\"Country\").alias(\"Country\"),\n",
    "    first(\"Abbreviation\").alias(\"Abbreviation\"),\n",
    "    first(\"Air_Temp_C\").alias(\"Air_Temp_C\"),\n",
    "    first(\"Track_Temp_C\").alias(\"Track_Temp_C\"),\n",
    "    first(\"Humidity_%\").alias(\"Humidity_%\"),\n",
    "    first(\"Wind_Speed_KMH\").alias(\"Wind_Speed_KMH\"),\n",
    "    first(\"AvgPitStopTime\").alias(\"AvgPitStopTime\"),\n",
    "    first(\"Pit_Lap\").alias(\"Pit_Lap\"),\n",
    "    first(\"Pit_Time\").alias(\"Pit_Time\"),\n",
    "    first(\"Tire Compound\").alias(\"Tire Compound\"),\n",
    "    first(\"Stint Length\").alias(\"Stint Length\")\n",
    ")\n",
    "\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9787130d-d933-45b7-92c7-88685920cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{OUT_DIR}/pitstop_aggregated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de0eb243-8a1e-41be-a11d-6992266c3320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety car DataFrame columns:\n",
      "root\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Cause: string (nullable = true)\n",
      " |-- Deployed: string (nullable = true)\n",
      " |-- Retreated: string (nullable = true)\n",
      " |-- FullLaps: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------+--------+---------+--------+\n",
      "|                Race|        Cause|Deployed|Retreated|FullLaps|\n",
      "+--------------------+-------------+--------+---------+--------+\n",
      "|1973 Canadian Gra...|     Accident|      33|       39|       5|\n",
      "|1993 Brazilian Gr...|Accident/Rain|      29|       38|       8|\n",
      "|1993 British Gran...| Stranded car|      38|       40|       1|\n",
      "|1994 San Marino G...|     Accident|       1|        6|       4|\n",
      "|1995 Belgian Gran...|         Rain|      28|       33|       4|\n",
      "+--------------------+-------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "Columns in agg_df: ['Season', 'Round', 'Circuit', 'Driver', 'Constructor', 'Race Name', 'Date', 'Location', 'Country', 'Abbreviation', 'Air_Temp_C', 'Track_Temp_C', 'Humidity_%', 'Wind_Speed_KMH', 'AvgPitStopTime', 'Pit_Lap', 'Pit_Time', 'Tire Compound', 'Stint Length']\n",
      "Columns in safety: ['Race', 'Cause', 'Deployed', 'Retreated', 'FullLaps']\n",
      "Common columns for joining: set()\n"
     ]
    }
   ],
   "source": [
    "# Check safety car DataFrame structure\n",
    "print(\"Safety car DataFrame columns:\")\n",
    "safety.printSchema()\n",
    "safety.show(5)\n",
    "\n",
    "# Check what join keys are available in both DataFrames\n",
    "print(\"Columns in agg_df:\", agg_df.columns)\n",
    "print(\"Columns in safety:\", safety.columns)\n",
    "\n",
    "# Find common columns for joining\n",
    "common_columns = set(agg_df.columns) & set(safety.columns)\n",
    "print(\"Common columns for joining:\", common_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5903df3a-0b87-4324-9bcf-fb156fb6aa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Race names from safety data:\n",
      "+-----------------------------+\n",
      "|Race                         |\n",
      "+-----------------------------+\n",
      "|2011 Belgian Grand Prix      |\n",
      "|1973 Canadian Grand Prix     |\n",
      "|2013 United States Grand Prix|\n",
      "|2016 British Grand Prix      |\n",
      "|2021 S√£o Paulo Grand Prix   |\n",
      "|2010 Belgian Grand Prix      |\n",
      "|2005 Brazilian Grand Prix    |\n",
      "|2020 Sakhir Grand Prix       |\n",
      "|2008 German Grand Prix       |\n",
      "|2009 Brazilian Grand Prix    |\n",
      "+-----------------------------+\n",
      "only showing top 10 rows\n",
      "Safety data with extracted columns:\n",
      "+--------------------------+-------------+--------+---------+--------+------+---------------------+\n",
      "|Race                      |Cause        |Deployed|Retreated|FullLaps|Season|Circuit              |\n",
      "+--------------------------+-------------+--------+---------+--------+------+---------------------+\n",
      "|1973 Canadian Grand Prix  |Accident     |33      |39       |5       |1973  |Canadian Grand Prix  |\n",
      "|1993 Brazilian Grand Prix |Accident/Rain|29      |38       |8       |1993  |Brazilian Grand Prix |\n",
      "|1993 British Grand Prix   |Stranded car |38      |40       |1       |1993  |British Grand Prix   |\n",
      "|1994 San Marino Grand Prix|Accident     |1       |6        |4       |1994  |San Marino Grand Prix|\n",
      "|1995 Belgian Grand Prix   |Rain         |28      |33       |4       |1995  |Belgian Grand Prix   |\n",
      "+--------------------------+-------------+--------+---------+--------+------+---------------------+\n",
      "only showing top 5 rows\n",
      "Joined DataFrame sample:\n",
      "+------+-----+--------------------+-------------------+-----+--------+\n",
      "|Season|Round|             Circuit|             Driver|Cause|Deployed|\n",
      "+------+-----+--------------------+-------------------+-----+--------+\n",
      "|  2018|    2|Bahrain Internati...|       Pierre Gasly| NULL|    NULL|\n",
      "|  2018|    4|   Baku City Circuit|       Carlos Sainz| NULL|    NULL|\n",
      "|  2018|    8| Circuit Paul Ricard|    Valtteri Bottas| NULL|    NULL|\n",
      "|  2018|   10| Silverstone Circuit|     Lewis Hamilton| NULL|    NULL|\n",
      "|  2018|   15|Marina Bay Street...|     Max Verstappen| NULL|    NULL|\n",
      "|  2019|    4|   Baku City Circuit|       Pierre Gasly| NULL|    NULL|\n",
      "|  2019|   13|Circuit de Spa-Fr...|Sergio P√É∆í√Ç¬©rez| NULL|    NULL|\n",
      "|  2020|    9|Autodromo Interna...|       Lando Norris| NULL|    NULL|\n",
      "|  2020|   15|Bahrain Internati...|   Daniel Ricciardo| NULL|    NULL|\n",
      "|  2021|   16|       Istanbul Park|   Sebastian Vettel| NULL|    NULL|\n",
      "+------+-----+--------------------+-------------------+-----+--------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Examine the Race column pattern in safety data\n",
    "print(\"Sample Race names from safety data:\")\n",
    "safety.select(\"Race\").distinct().show(10, truncate=False)\n",
    "\n",
    "# If Race names follow a pattern like \"2023 Monaco Grand Prix\", we can extract Season\n",
    "from pyspark.sql.functions import split, col, regexp_extract\n",
    "\n",
    "# Try to extract season year from Race name\n",
    "safety_with_season = safety.withColumn(\n",
    "    \"Season\", \n",
    "    regexp_extract(col(\"Race\"), r\"(\\d{4})\", 1).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Try to extract circuit name (everything after the year)\n",
    "safety_with_circuit = safety_with_season.withColumn(\n",
    "    \"Circuit\",\n",
    "    regexp_extract(col(\"Race\"), r\"\\d{4} (.*)\", 1)\n",
    ")\n",
    "\n",
    "print(\"Safety data with extracted columns:\")\n",
    "safety_with_circuit.show(5, truncate=False)\n",
    "\n",
    "# Now join with the extracted columns\n",
    "final_df = agg_df.join(\n",
    "    safety_with_circuit, \n",
    "    [\"Season\", \"Circuit\"], \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"Joined DataFrame sample:\")\n",
    "final_df.select(\"Season\", \"Round\", \"Circuit\", \"Driver\", \"Cause\", \"Deployed\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa74461d-f5ad-4e4a-9fd2-2a64a17754df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total safety car events in dataset: 362\n",
      "Races with safety car deployments: 0\n",
      "Races with safety car deployments:\n",
      "+------+-----+-------+------+-----+--------+---------+\n",
      "|Season|Round|Circuit|Driver|Cause|Deployed|Retreated|\n",
      "+------+-----+-------+------+-----+--------+---------+\n",
      "+------+-----+-------+------+-----+--------+---------+\n",
      "\n",
      "Data processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check how many races had safety car deployments\n",
    "safety_count = safety_with_circuit.count()\n",
    "print(f\"Total safety car events in dataset: {safety_count}\")\n",
    "\n",
    "# Check which races in agg_df matched with safety car data\n",
    "matches = final_df.filter(col(\"Cause\").isNotNull()).count()\n",
    "print(f\"Races with safety car deployments: {matches}\")\n",
    "\n",
    "# Show some races that actually had safety cars\n",
    "print(\"Races with safety car deployments:\")\n",
    "final_df.filter(col(\"Cause\").isNotNull()).select(\n",
    "    \"Season\", \"Round\", \"Circuit\", \"Driver\", \"Cause\", \"Deployed\", \"Retreated\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Continue with your analysis - you now have a complete dataset\n",
    "# Save the final joined data\n",
    "final_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{OUT_DIR}/final_joined_data\")\n",
    "\n",
    "print(\"Data processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f80221f-f835-4039-b647-ea155c259747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season range in pitstop data:\n",
      "+----------+----------+\n",
      "|min_season|max_season|\n",
      "+----------+----------+\n",
      "|      2018|      2024|\n",
      "+----------+----------+\n",
      "\n",
      "Season range in safety car data:\n",
      "+----------+----------+\n",
      "|min_season|max_season|\n",
      "+----------+----------+\n",
      "|      1973|      2025|\n",
      "+----------+----------+\n",
      "\n",
      "Pitstop seasons: [2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Safety car seasons: [1973, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
      "Overlapping seasons: {2018, 2019, 2020, 2021, 2022, 2023, 2024}\n"
     ]
    }
   ],
   "source": [
    "# Check season ranges in both datasets\n",
    "print(\"Season range in pitstop data:\")\n",
    "agg_df.select(\n",
    "    F.min(\"Season\").alias(\"min_season\"), \n",
    "    F.max(\"Season\").alias(\"max_season\")\n",
    ").show()\n",
    "\n",
    "print(\"Season range in safety car data:\")\n",
    "safety_with_circuit.select(\n",
    "    F.min(\"Season\").alias(\"min_season\"), \n",
    "    F.max(\"Season\").alias(\"max_season\")\n",
    ").show()\n",
    "\n",
    "# Check if there are any overlapping seasons\n",
    "pitstop_seasons = agg_df.select(\"Season\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "safety_seasons = safety_with_circuit.select(\"Season\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(f\"Pitstop seasons: {sorted(pitstop_seasons)}\")\n",
    "print(f\"Safety car seasons: {sorted(safety_seasons)}\")\n",
    "print(f\"Overlapping seasons: {set(pitstop_seasons) & set(safety_seasons)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56e3e846-daa5-4b33-b195-e47d22452285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample circuit names from pitstop data:\n",
      "+------------------------------------------+\n",
      "|Circuit                                   |\n",
      "+------------------------------------------+\n",
      "|Istanbul Park                             |\n",
      "|Aut√É∆í√Ç¬≥dromo Internacional do Algarve |\n",
      "|Albert Park Grand Prix Circuit            |\n",
      "|Circuit Gilles Villeneuve                 |\n",
      "|Suzuka Circuit                            |\n",
      "|Aut√É∆í√Ç¬≥dromo Hermanos Rodr√É∆í√Ç¬≠guez|\n",
      "|Miami International Autodrome             |\n",
      "|Autodromo Nazionale di Monza              |\n",
      "|Baku City Circuit                         |\n",
      "|Silverstone Circuit                       |\n",
      "+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "Sample circuit names from safety car data:\n",
      "+---------------------+\n",
      "|Circuit              |\n",
      "+---------------------+\n",
      "|Sakhir Grand Prix    |\n",
      "|S√£o Paulo Grand Prix|\n",
      "|German Grand Prix    |\n",
      "|Spanish Grand Prix   |\n",
      "|Turkish Grand Prix   |\n",
      "|Miami Grand Prix     |\n",
      "|Brazilian Grand Prix |\n",
      "|Argentine Grand Prix |\n",
      "|Mexican Grand Prix   |\n",
      "|Portuguese Grand Prix|\n",
      "+---------------------+\n",
      "only showing top 10 rows\n",
      "Exact matches found: 0\n",
      "Pitstop circuits (2018-2024): ['suzuka circuit', 'istanbul park', 'autodromo nazionale di monza', 'sochi autodrom', 'shanghai international circuit', 'circuit de barcelona-catalunya', 'circuit de spa-francorchamps', 'circuit de monaco', 'autodromo internazionale del mugello', 'red bull ring', 'autodromo enzo e dino ferrari', 'yas marina circuit', 'bahrain international circuit', 'las vegas strip street circuit', 'circuit park zandvoort', 'circuit paul ricard', 'hungaroring', 'silverstone circuit', 'aut√é∆í√ç¬≥dromo internacional do algarve', 'circuit of the americas', 'hockenheimring', 'jeddah corniche circuit', 'marina bay street circuit', 'circuit gilles villeneuve', 'n√é∆í√ç¬ºrburgring', 'miami international autodrome', 'aut√é∆í√ç¬≥dromo hermanos rodr√é∆í√ç¬≠guez', 'albert park grand prix circuit', 'losail international circuit', 'aut√é∆í√ç¬≥dromo jos√é∆í√ç¬© carlos pace', 'baku city circuit']\n",
      "Safety car circuits (2018-2024): ['spanish grand prix', 'japanese grand prix', 'qatar grand prix', 'austrian grand prix', 'dutch grand prix', 'tuscan grand prix', 'chinese grand prix', 'german grand prix', 'italian grand prix', 'eifel grand prix', 'brazilian grand prix', 'belgian grand prix', 'sakhir grand prix', 'british grand prix', 'azerbaijan grand prix', 'bahrain grand prix', 'canadian grand prix', 'las vegas grand prix', 'singapore grand prix', 's√£o paulo grand prix', 'abu dhabi grand prix', 'french grand prix', 'saudi arabian grand prix', 'mexico city grand prix', 'australian grand prix', 'portuguese grand prix', 'united states grand prix', 'miami grand prix', 'hungarian grand prix', 'russian grand prix', 'emilia romagna grand prix', 'monaco grand prix', 'styrian grand prix']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check circuit name differences between datasets\n",
    "print(\"Sample circuit names from pitstop data:\")\n",
    "agg_df.select(\"Circuit\").distinct().show(10, truncate=False)\n",
    "\n",
    "print(\"Sample circuit names from safety car data:\")\n",
    "safety_with_circuit.select(\"Circuit\").distinct().show(10, truncate=False)\n",
    "\n",
    "# Check for exact matches on overlapping seasons\n",
    "overlap_df = agg_df.join(\n",
    "    safety_with_circuit.filter(F.col(\"Season\").between(2018, 2024)),\n",
    "    [\"Season\", \"Circuit\"], \n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Exact matches found: {overlap_df.count()}\")\n",
    "\n",
    "# Let's try a fuzzy match by checking partial circuit name matches\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "# Normalize circuit names and check for partial matches\n",
    "pitstop_circuits = agg_df.select(\n",
    "    F.lower(F.col(\"Circuit\")).alias(\"circuit_norm\")\n",
    ").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "safety_circuits = safety_with_circuit.filter(F.col(\"Season\").between(2018, 2024)).select(\n",
    "    F.lower(F.col(\"Circuit\")).alias(\"circuit_norm\")\n",
    ").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(f\"Pitstop circuits (2018-2024): {pitstop_circuits}\")\n",
    "print(f\"Safety car circuits (2018-2024): {safety_circuits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3b1a119-ff50-4ead-9a72-077a02695a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 1940\n",
      "+------+--------------+-------------------+--------------------+\n",
      "|Season|Circuit       |Race_Name          |Cause               |\n",
      "+------+--------------+-------------------+--------------------+\n",
      "|2018  |Suzuka Circuit|Japanese Grand Prix|Debris from accident|\n",
      "|2018  |Suzuka Circuit|Japanese Grand Prix|Debris from accident|\n",
      "|2022  |Suzuka Circuit|Japanese Grand Prix|Stranded car/Rain   |\n",
      "|2018  |Suzuka Circuit|Japanese Grand Prix|Debris from accident|\n",
      "|2018  |Suzuka Circuit|Japanese Grand Prix|Debris from accident|\n",
      "|2023  |Suzuka Circuit|Japanese Grand Prix|Debris from accident|\n",
      "|2022  |Suzuka Circuit|Japanese Grand Prix|Stranded car/Rain   |\n",
      "|2022  |Suzuka Circuit|Japanese Grand Prix|Stranded car/Rain   |\n",
      "|2018  |Suzuka Circuit|Japanese Grand Prix|Debris from accident|\n",
      "|2018  |Suzuka Circuit|Japanese Grand Prix|Debris from accident|\n",
      "+------+--------------+-------------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Create a circuit to race name mapping\n",
    "circuit_mapping = {\n",
    "    \"Suzuka Circuit\": \"Japanese Grand Prix\",\n",
    "    \"Istanbul Park\": \"Turkish Grand Prix\", \n",
    "    \"Autodromo Nazionale di Monza\": \"Italian Grand Prix\",\n",
    "    \"Circuit de Barcelona-Catalunya\": \"Spanish Grand Prix\",\n",
    "    \"Circuit de Spa-Francorchamps\": \"Belgian Grand Prix\",\n",
    "    \"Circuit de Monaco\": \"Monaco Grand Prix\",\n",
    "    \"Red Bull Ring\": \"Austrian Grand Prix\",\n",
    "    \"Yas Marina Circuit\": \"Abu Dhabi Grand Prix\",\n",
    "    \"Bahrain International Circuit\": \"Bahrain Grand Prix\",\n",
    "    \"Circuit Park Zandvoort\": \"Dutch Grand Prix\",\n",
    "    \"Circuit Paul Ricard\": \"French Grand Prix\",\n",
    "    \"Hungaroring\": \"Hungarian Grand Prix\",\n",
    "    \"Silverstone Circuit\": \"British Grand Prix\",\n",
    "    \"Circuit of the Americas\": \"United States Grand Prix\",\n",
    "    \"Hockenheimring\": \"German Grand Prix\",\n",
    "    \"Jeddah Corniche Circuit\": \"Saudi Arabian Grand Prix\",\n",
    "    \"Marina Bay Street Circuit\": \"Singapore Grand Prix\",\n",
    "    \"Circuit Gilles Villeneuve\": \"Canadian Grand Prix\",\n",
    "    \"Miami International Autodrome\": \"Miami Grand Prix\",\n",
    "    \"Autódromo José Carlos Pace\": \"São Paulo Grand Prix\",\n",
    "    \"Albert Park Grand Prix Circuit\": \"Australian Grand Prix\",\n",
    "    \"Baku City Circuit\": \"Azerbaijan Grand Prix\",\n",
    "    \"Losail International Circuit\": \"Qatar Grand Prix\",\n",
    "    \"Sochi Autodrom\": \"Russian Grand Prix\"\n",
    "}\n",
    "\n",
    "mapping_df = spark.createDataFrame([(k, v) for k, v in circuit_mapping.items()], [\"Circuit\", \"Race_Name\"])\n",
    "\n",
    "# Join pitstop data with mapping\n",
    "agg_df_with_race = agg_df.join(mapping_df, \"Circuit\", \"left\")\n",
    "\n",
    "# Now continue with your join\n",
    "final_df = agg_df_with_race.join(\n",
    "    safety_with_race, \n",
    "    [\"Season\", \"Race_Name\"], \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Rename the Circuit column in safety data to match our mapping\n",
    "safety_with_race = safety_with_circuit.withColumnRenamed(\"Circuit\", \"Race_Name\")\n",
    "\n",
    "# Now join with safety data using the mapped race names\n",
    "final_df = agg_df_with_race.join(\n",
    "    safety_with_race, \n",
    "    [\"Season\", \"Race_Name\"], \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(f\"Matches found: {final_df.filter(F.col('Cause').isNotNull()).count()}\")\n",
    "final_df.filter(F.col('Cause').isNotNull()).select(\"Season\", \"Circuit\", \"Race_Name\", \"Cause\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a27e40e-2687-4aa6-ad99-1d46ac144766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-----+------------+-------+\n",
      "|      pit2_id|Season|Round|Abbreviation|Pit_Lap|\n",
      "+-------------+------+-----+------------+-------+\n",
      "|2018_1_HAR_22|  2018|    1|         HAR|     22|\n",
      "|2018_1_SAI_22|  2018|    1|         SAI|     22|\n",
      "|2018_1_LEC_20|  2018|    1|         LEC|     20|\n",
      "|2018_1_RIC_26|  2018|    1|         RIC|     26|\n",
      "|2018_1_OCO_23|  2018|    1|         OCO|     23|\n",
      "+-------------+------+-----+------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "# Apply to final_df instead of agg_df\n",
    "if \"Pit_Lap\" in final_df.columns:\n",
    "    final_df = final_df.withColumn(\"Pit_Lap\", col(\"Pit_Lap\").cast(IntegerType()))\n",
    "\n",
    "# Create canonical ID (note: no Stint column, using Pit_Lap instead)\n",
    "final_df = final_df.withColumn(\n",
    "    \"pit2_id\",\n",
    "    concat_ws(\"_\", \n",
    "        col(\"Season\").cast(StringType()), \n",
    "        col(\"Round\").cast(StringType()), \n",
    "        col(\"Abbreviation\"), \n",
    "        col(\"Pit_Lap\").cast(StringType())  # Using Pit_Lap instead of Stint\n",
    "    )\n",
    ")\n",
    "\n",
    "final_df.select(\"pit2_id\", \"Season\", \"Round\", \"Abbreviation\", \"Pit_Lap\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4caf53c-8479-4386-8ea3-12b4d4f45307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Final dataset saved!\n",
      "Dataset contains 3381 rows\n",
      "Safety car events matched: 1940\n",
      "root\n",
      " |-- Season: integer (nullable = true)\n",
      " |-- Race_Name: string (nullable = true)\n",
      " |-- Circuit: string (nullable = true)\n",
      " |-- Round: integer (nullable = true)\n",
      " |-- Driver: string (nullable = true)\n",
      " |-- Constructor: string (nullable = true)\n",
      " |-- Race Name: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Abbreviation: string (nullable = true)\n",
      " |-- Air_Temp_C: string (nullable = true)\n",
      " |-- Track_Temp_C: string (nullable = true)\n",
      " |-- Humidity_%: string (nullable = true)\n",
      " |-- Wind_Speed_KMH: string (nullable = true)\n",
      " |-- AvgPitStopTime: string (nullable = true)\n",
      " |-- Pit_Lap: integer (nullable = true)\n",
      " |-- Pit_Time: string (nullable = true)\n",
      " |-- Tire Compound: string (nullable = true)\n",
      " |-- Stint Length: integer (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Cause: string (nullable = true)\n",
      " |-- Deployed: string (nullable = true)\n",
      " |-- Retreated: string (nullable = true)\n",
      " |-- FullLaps: string (nullable = true)\n",
      " |-- pit2_id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save your current final dataset\n",
    "final_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{OUT_DIR}/final_pitstop_analysis_data.csv\")\n",
    "\n",
    "print(\"SUCCESS: Final dataset saved!\")\n",
    "print(f\"Dataset contains {final_df.count()} rows\")\n",
    "print(f\"Safety car events matched: {final_df.filter(F.col('Cause').isNotNull()).count()}\")\n",
    "\n",
    "# Show final schema\n",
    "final_df.printSchema()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8b798-d3ed-4d7b-8e7b-0a452c95d874",
   "metadata": {},
   "source": [
    "Now our data cleaing and processing is done we will get all the dataset with our requirment satisfied "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc3683-e028-4065-a4f6-087370ce2807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
