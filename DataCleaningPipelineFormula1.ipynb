{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ab1759-b085-4b3a-80b4-e802d1ee6aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/21 14:58:54 WARN Utils: Your hostname, Divyanshs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.33.74.20 instead (on interface en0)\n",
      "25/11/21 14:58:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/21 14:58:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, trim, lower, when, coalesce, first, last,\n",
    "    concat_ws, concat, udf, collect_list, array_join, explode,\n",
    "    min as spark_min, max as spark_max, expr\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# ========== 1. Start spark ==========\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F1 Pitstop Pipeline 2018-2024\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb439eb3-ac44-45f6-b726-75dcda84c22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "doneecting package metadata (repodata.json): - \n",
      "doneing environment: / \n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.9.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge openjdk=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add63462-187e-4ddb-ac20-65b60651c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. Paths (update to your repo / S3 / HDFS) ==========\n",
    "RAW_DIR = \"./data/raw\"           # relative to current directory\n",
    "OUT_DIR = \"./data/processed\"     # final outputs here\n",
    "\n",
    "# example files:\n",
    "PIT1_PATH = f\"{RAW_DIR}/pitstop_1st.csv\"\n",
    "PIT2_PATH = f\"{RAW_DIR}/pitstop_2nd.csv\"\n",
    "SC_PATH   = f\"{RAW_DIR}/safety_cars.csv\"\n",
    "RF_PATH   = f\"{RAW_DIR}/red_flags.csv\"\n",
    "\n",
    "# optional lookups (you should download from Ergast/Kaggle or supply them)\n",
    "RACES_PATH   = f\"{RAW_DIR}/races.csv\"    # maps raceId -> Season, Round, RaceName, Date\n",
    "DRIVERS_PATH = f\"{RAW_DIR}/drivers.csv\"  # maps driverId -> driverName, abbreviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc51d47-013b-4aef-8137-6cbc5a6aa888",
   "metadata": {},
   "source": [
    "Now Checking if the files are loaded correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7376f639-5e45-4bf5-ba05-be65182b5a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/divyanshdoshi/Documents/GitHub/Cloud_Formula1_Data_Cleaning_Pipeline\n",
      "\n",
      "Checking file existence:\n",
      "yes Pitstops 1st: ./data/raw/pitstop_1st.csv\n",
      "yes Pitstops 2nd: ./data/raw/pitstop_2nd.csv\n",
      "yes Safety Car: ./data/raw/safety_cars.csv\n",
      "yes Red Flag: ./data/raw/red_flags.csv\n",
      "yes Races Lookup: ./data/raw/races.csv\n",
      "yes Drivers Lookup: ./data/raw/drivers.csv\n",
      "\n",
      "Contents of ./data/raw:\n",
      "  - pitstop_1st.csv\n",
      "  - safety_cars.csv\n",
      "  - drivers.csv\n",
      "  - red_flags.csv\n",
      "  - races.csv\n",
      "  - pitstop_2nd.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nChecking file existence:\")\n",
    "files_to_check = {\n",
    "    \"Pitstops 1st\": PIT1_PATH,\n",
    "    \"Pitstops 2nd\": PIT2_PATH,\n",
    "    \"Safety Car\": SC_PATH,\n",
    "    \"Red Flag\": RF_PATH,\n",
    "    \"Races Lookup\": RACES_PATH,\n",
    "    \"Drivers Lookup\": DRIVERS_PATH\n",
    "}\n",
    "\n",
    "for name, path in files_to_check.items():\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"{'yes' if exists else 'no'} {name}: {path}\")\n",
    "    \n",
    "# Also check the raw directory contents\n",
    "print(f\"\\nContents of {RAW_DIR}:\")\n",
    "try:\n",
    "    if os.path.exists(RAW_DIR):\n",
    "        for item in os.listdir(RAW_DIR):\n",
    "            print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(f\"Directory {RAW_DIR} does not exist!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fa805-d70b-4796-b0f9-cfdd0975979b",
   "metadata": {},
   "source": [
    "Now We can see that the dataset are correctly loaded now we can start cleaning and joining the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc448c-2759-41c1-a172-486249d020a9",
   "metadata": {},
   "source": [
    "Our main aim of this Pipeline is to get out clean pitstop there are many problems with the dataset like name issues and no data in someplace so we will tackle this using spark \n",
    "And for our final aim to use this data for predicting better pitstop strategy we all need to know when the pitstop was done under safety car which can change whole race dynamic and we have to change the whole race strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a1dca43-343d-4b5f-9cfd-9304cdb75d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use inferSchema=False and define schema or let Spark infer; define options to handle encoding.\n",
    "pit2 = spark.read.options(header=True, multiLine=True, escape='\"').csv(PIT2_PATH)\n",
    "pit1 = spark.read.options(header=True, multiLine=True, escape='\"').csv(PIT1_PATH)\n",
    "safety = spark.read.options(header=True).csv(SC_PATH)\n",
    "redflag = spark.read.options(header=True).csv(RF_PATH)\n",
    "\n",
    "# Optional lookups (if present)\n",
    "races_df = None\n",
    "drivers_df = None\n",
    "try:\n",
    "    races_df = spark.read.options(header=True).csv(RACES_PATH)\n",
    "    drivers_df = spark.read.options(header=True).csv(DRIVERS_PATH)\n",
    "except Exception:\n",
    "    print(\"Lookups not provided yet; pipeline will attempt fuzzy joins or require you to add lookups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a777692-b69e-4088-a13b-55761e200a5c",
   "metadata": {},
   "source": [
    "This code loads your raw data files (pit stops, safety car, red flag data, races, drivers) into Spark DataFrames so we can process them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4578c1-b8f0-4a6a-9199-fa7e5f2b9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(df, col_names):\n",
    "    # Trim whitespace and convert empty strings to null\n",
    "    for c in col_names:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "            df = df.withColumn(c, when(col(c) == \"\", None).otherwise(col(c)))\n",
    "    return df\n",
    "\n",
    "pit2 = basic_clean(pit2, pit2.columns)\n",
    "pit1 = basic_clean(pit1, pit1.columns)\n",
    "safety = basic_clean(safety, safety.columns)\n",
    "redflag = basic_clean(redflag, redflag.columns)\n",
    "\n",
    "encoding_fixes = {\n",
    "    \"Kimi R√É∆í√Ç¬§ikk√É∆í√Ç¬∂nen\": \"Kimi Räikkönen\",\n",
    "    \"RÃ¤ikkÃ¶nen\": \"Kimi Räikkönen\"\n",
    "}\n",
    "fix_udf = F.udf(lambda s: encoding_fixes.get(s, s) if s is not None else None, StringType())\n",
    "if \"Driver\" in pit2.columns:\n",
    "    pit2 = pit2.withColumn(\"Driver\", fix_udf(col(\"Driver\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a88e82-d419-4b07-9c00-f33795c4bd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
